#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ivod_tasks.py

å°‡ fullã€incrementalã€retry ä¸‰ç¨®ä¸»è¦å·¥ä½œæµç¨‹é›†ä¸­åœ¨æ­¤ï¼Œä¾› ivod_full.pyã€ivod_incremental.pyã€ivod_retry.py å‘¼å«ã€‚
"""
import logging
from datetime import datetime, timedelta
import os
from pathlib import Path

from tqdm import tqdm
try:
    from elasticsearch import Elasticsearch
except ImportError:
    Elasticsearch = None

from .core import date_range, make_browser, fetch_ivod_list, process_ivod, Session, IVODTranscript
from .db import DB_BACKEND, engine, Base

logger = logging.getLogger(__name__)

def check_and_create_database_tables():
    """
    æª¢æŸ¥è³‡æ–™åº«é€£ç·šç‹€æ³ä¸¦ç¢ºä¿è¡¨æ ¼å­˜åœ¨
    """
    from sqlalchemy import inspect, text
    
    try:
        # æª¢æŸ¥è³‡æ–™åº«é€£ç·š
        with engine.connect() as conn:
            result = conn.execute(text("SELECT 1"))
            logger.info(f"âœ… è³‡æ–™åº«é€£ç·šæˆåŠŸ (Backend: {DB_BACKEND})")
        
        # æª¢æŸ¥è¡¨æ ¼æ˜¯å¦å­˜åœ¨
        inspector = inspect(engine)
        tables = inspector.get_table_names()
        
        if 'ivod_transcripts' not in tables:
            logger.info("âš ï¸  ivod_transcripts è¡¨æ ¼ä¸å­˜åœ¨ï¼Œæ­£åœ¨å‰µå»º...")
            Base.metadata.create_all(engine)
            logger.info("âœ… è¡¨æ ¼å‰µå»ºæˆåŠŸ")
        else:
            logger.info("âœ… ivod_transcripts è¡¨æ ¼å·²å­˜åœ¨")
            
            # æª¢æŸ¥è¡¨æ ¼çµæ§‹
            columns = inspector.get_columns('ivod_transcripts')
            logger.info(f"âœ… è¡¨æ ¼åŒ…å« {len(columns)} å€‹æ¬„ä½")
            
            # æª¢æŸ¥ç¾æœ‰è¨˜éŒ„æ•¸
            with Session() as session:
                count = session.query(IVODTranscript).count()
                logger.info(f"âœ… ç¾æœ‰è¨˜éŒ„æ•¸: {count}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ è³‡æ–™åº«æª¢æŸ¥å¤±æ•—: {e}")
        return False

def log_failed_ivod(ivod_id, error_type="general"):
    """è¨˜éŒ„å¤±æ•—çš„IVOD_IDåˆ°éŒ¯èª¤æ—¥èªŒæª”æ¡ˆ"""
    error_log_path = os.getenv("ERROR_LOG_PATH", "logs/failed_ivods.txt")
    error_dir = Path(error_log_path).parent
    error_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(error_log_path, "a", encoding="utf-8") as f:
        f.write(f"{ivod_id},{error_type},{timestamp}\n")

def setup_logging():
    """è¨­ç½®æ—¥èªŒé…ç½® - æˆåŠŸæ¶ˆæ¯åªè¨˜éŒ„åˆ°æ–‡ä»¶ï¼ŒéŒ¯èª¤æ¶ˆæ¯åŒæ™‚é¡¯ç¤ºåœ¨æ§åˆ¶å°å’Œè¨˜éŒ„åˆ°æ–‡ä»¶"""
    log_path = os.getenv("LOG_PATH", "logs/")
    log_dir = Path(log_path)
    log_dir.mkdir(exist_ok=True)
    
    log_file = log_dir / f"crawler_{datetime.now().strftime('%Y%m%d')}.log"
    
    # æ¸…é™¤ç¾æœ‰çš„handlersä»¥é¿å…é‡è¤‡è¨­ç½®
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # å‰µå»ºæ–‡ä»¶handlerï¼Œè¨˜éŒ„æ‰€æœ‰ç´šåˆ¥çš„æ—¥èªŒ
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s")
    file_handler.setFormatter(file_formatter)
    
    # å‰µå»ºæ§åˆ¶å°handlerï¼Œåªé¡¯ç¤ºERRORå’ŒWARNINGç´šåˆ¥
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.WARNING)
    console_formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s")
    console_handler.setFormatter(console_formatter)
    
    # é…ç½®root logger
    logging.basicConfig(
        level=logging.INFO,
        handlers=[file_handler, console_handler]
    )


def run_full(skip_ssl: bool = True):
    """
    å…¨é‡æ‹‰å–ï¼šå¾å›ºå®šèµ·å§‹æ—¥è·‘åˆ°ä»Šå¤©ï¼Œé€ç­† upsert åˆ°è³‡æ–™åº«ã€‚
    """
    setup_logging()
    
    # æª¢æŸ¥ä¸¦ç¢ºä¿è³‡æ–™åº«è¡¨æ ¼å­˜åœ¨
    logger.info("ğŸ” æª¢æŸ¥è³‡æ–™åº«ç‹€æ³...")
    if not check_and_create_database_tables():
        logger.error("âŒ è³‡æ–™åº«æª¢æŸ¥å¤±æ•—ï¼Œåœæ­¢åŸ·è¡Œ")
        return False
    
    br = make_browser(skip_ssl=skip_ssl)
    db = Session()

    start, end = "2024-02-01", datetime.now().strftime("%Y-%m-%d")
    for date_str in tqdm(date_range(start, end), desc="æ—¥æœŸ"):
        try:
            ids = fetch_ivod_list(br, date_str)
        except Exception as e:
            logger.error(f"{date_str} åˆ—è¡¨å¤±æ•—: {e}")
            continue

        for ivod_id in tqdm(ids, desc=f"{date_str} å½±ç‰‡", leave=False):
            try:
                logger.info(f"è™•ç†å½±ç‰‡ {ivod_id}")
                rec = process_ivod(br, ivod_id)
                
                # Check if record exists
                obj = db.query(IVODTranscript).filter_by(ivod_id=ivod_id).first()
                if obj:
                    # Update existing record
                    for k, v in rec.items():
                        setattr(obj, k, v)
                    obj.last_updated = datetime.now()
                else:
                    # Create new record
                    rec["last_updated"] = datetime.now()
                    db.add(IVODTranscript(**rec))
                
                # Commit this single record
                db.commit()
                logger.info(f"å½±ç‰‡ {ivod_id} è™•ç†å®Œæˆ")
                
            except Exception as e:
                logger.error(f"è™•ç†å½±ç‰‡ {ivod_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
                # Rollback any pending changes for this record
                db.rollback()
                log_failed_ivod(ivod_id, "processing")
                continue
    db.close()
    logger.info("å…¨é‡æ‹‰å–å®Œæˆã€‚")
    
    # æª¢æŸ¥ Elasticsearch æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœå¯ç”¨å°±è‡ªå‹•æ›´æ–°ç´¢å¼•
    if check_elasticsearch_available():
        logger.info("ğŸ”„ é–‹å§‹è‡ªå‹•æ›´æ–° Elasticsearch ç´¢å¼•...")
        try:
            es_success = run_es(full_mode=True)
            if es_success:
                logger.info("âœ… Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°å®Œæˆ")
            else:
                logger.warning("âš ï¸  Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°å¤±æ•—")
        except Exception as e:
            logger.warning(f"âš ï¸  Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    return True


def run_incremental(skip_ssl: bool = True):
    """
    å¢é‡æ›´æ–°ï¼šåªæª¢æŸ¥éå»å…©é€±çš„æ–° IDï¼Œä¸¦é‡å°ç¼ºæ¼çš„ AI æˆ– LY é€å­—ç¨¿é€²è¡Œè£œæŠ“ã€‚
    """
    setup_logging()
    
    # æª¢æŸ¥ä¸¦ç¢ºä¿è³‡æ–™åº«è¡¨æ ¼å­˜åœ¨
    logger.info("ğŸ” æª¢æŸ¥è³‡æ–™åº«ç‹€æ³...")
    if not check_and_create_database_tables():
        logger.error("âŒ è³‡æ–™åº«æª¢æŸ¥å¤±æ•—ï¼Œåœæ­¢åŸ·è¡Œ")
        return False
    
    br = make_browser(skip_ssl=skip_ssl)
    db = Session()

    today = datetime.now().date()
    two_weeks_ago = today - timedelta(days=14)
    ids = set()
    for d in date_range(two_weeks_ago.isoformat(), today.isoformat()):
        try:
            ids.update(fetch_ivod_list(br, d))
        except Exception:
            continue

    for ivod_id in tqdm(ids, desc="å¢é‡æ›´æ–°å½±ç‰‡"):
        try:
            logger.info(f"å¢é‡æ›´æ–°å½±ç‰‡ {ivod_id}")
            obj = db.get(IVODTranscript, ivod_id)
            if not obj:
                rec = process_ivod(br, ivod_id)
                rec["last_updated"] = datetime.now()
                db.add(IVODTranscript(**rec))
                logger.info(f"æ–°å¢å½±ç‰‡ {ivod_id}")
                continue
            if not obj.ai_transcript:
                rec = process_ivod(br, ivod_id)
                obj.ai_transcript = rec["ai_transcript"]
                obj.last_updated = datetime.now()
                logger.info(f"æ›´æ–°å½±ç‰‡ {ivod_id} AIé€å­—ç¨¿")
            if not obj.ly_transcript:
                rec = process_ivod(br, ivod_id)
                obj.ly_transcript = rec["ly_transcript"]
                obj.last_updated = datetime.now()
                logger.info(f"æ›´æ–°å½±ç‰‡ {ivod_id} LYé€å­—ç¨¿")
        except Exception as e:
            logger.error(f"å¢é‡æ›´æ–°å½±ç‰‡ {ivod_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            log_failed_ivod(ivod_id, "incremental")
            continue

    db.commit()
    db.close()
    logger.info("å¢é‡æ›´æ–°å®Œæˆã€‚")
    
    # æª¢æŸ¥ Elasticsearch æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœå¯ç”¨å°±è‡ªå‹•æ›´æ–°ç´¢å¼•ï¼ˆå¢é‡æ¨¡å¼ï¼‰
    if check_elasticsearch_available():
        logger.info("ğŸ”„ é–‹å§‹è‡ªå‹•æ›´æ–° Elasticsearch ç´¢å¼•ï¼ˆå¢é‡æ¨¡å¼ï¼‰...")
        try:
            es_success = run_es()  # ä½¿ç”¨é è¨­å¢é‡æ¨¡å¼
            if es_success:
                logger.info("âœ… Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°å®Œæˆ")
            else:
                logger.warning("âš ï¸  Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°å¤±æ•—")
        except Exception as e:
            logger.warning(f"âš ï¸  Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    return True


def run_retry(skip_ssl: bool = True):
    """
    é‡æ–°å˜—è©¦å¤±æ•—çš„ä»»å‹™ï¼šAI æˆ– LY é€å­—ç¨¿ä¹‹å‰ç™¼ç”ŸéŒ¯èª¤ï¼Œä¸”é‡è©¦æ¬¡æ•¸å°šæœªè¶…éä¸Šé™ã€‚
    """
    setup_logging()
    
    # æª¢æŸ¥ä¸¦ç¢ºä¿è³‡æ–™åº«è¡¨æ ¼å­˜åœ¨
    logger.info("ğŸ” æª¢æŸ¥è³‡æ–™åº«ç‹€æ³...")
    if not check_and_create_database_tables():
        logger.error("âŒ è³‡æ–™åº«æª¢æŸ¥å¤±æ•—ï¼Œåœæ­¢åŸ·è¡Œ")
        return False
    
    br = make_browser(skip_ssl=skip_ssl)
    db = Session()

    MAX_RETRIES = 5
    to_retry = db.query(IVODTranscript).filter(
        IVODTranscript.ai_status == 'failed',
        IVODTranscript.ai_retries < MAX_RETRIES
    ).all()
    to_retry += db.query(IVODTranscript).filter(
        IVODTranscript.ly_status == 'failed',
        IVODTranscript.ly_retries < MAX_RETRIES
    ).all()

    # è¨˜éŒ„æˆåŠŸé‡è©¦çš„ IVOD IDs
    successfully_retried_ids = []

    for obj in to_retry:
        try:
            logger.info(f"é‡è©¦å½±ç‰‡ {obj.ivod_id}")
            rec = process_ivod(br, obj.ivod_id)
            # Update the existing object with new data
            for k, v in rec.items():
                setattr(obj, k, v)
            obj.last_updated = datetime.now()
            db.commit()
            successfully_retried_ids.append(obj.ivod_id)
            logger.info(f"é‡è©¦å½±ç‰‡ {obj.ivod_id} å®Œæˆ")
        except Exception as e:
            logger.error(f"é‡è©¦å½±ç‰‡ {obj.ivod_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            log_failed_ivod(obj.ivod_id, "retry")
            continue

    db.close()
    logger.info("Retry ä»»å‹™å®Œæˆã€‚")
    
    # æª¢æŸ¥ Elasticsearch æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœå¯ç”¨ä¸”æœ‰æˆåŠŸé‡è©¦çš„è¨˜éŒ„å°±è‡ªå‹•æ›´æ–°ç´¢å¼•
    if successfully_retried_ids and check_elasticsearch_available():
        logger.info(f"ğŸ”„ é–‹å§‹è‡ªå‹•æ›´æ–° Elasticsearch ç´¢å¼•ï¼ˆé‡è©¦çš„ {len(successfully_retried_ids)} ç­†è¨˜éŒ„ï¼‰...")
        try:
            es_success = run_es(ivod_ids=successfully_retried_ids)
            if es_success:
                logger.info("âœ… Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°å®Œæˆ")
            else:
                logger.warning("âš ï¸  Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°å¤±æ•—")
        except Exception as e:
            logger.warning(f"âš ï¸  Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    elif successfully_retried_ids:
        logger.info(f"â„¹ï¸  å·²é‡è©¦ {len(successfully_retried_ids)} ç­†è¨˜éŒ„ï¼Œä½† Elasticsearch ä¸å¯ç”¨")
    
    return True


def check_elasticsearch_available():
    """
    æª¢æŸ¥ Elasticsearch æ˜¯å¦å¯ç”¨
    è¿”å› True å¦‚æœ ES æ­£å¸¸é‹ä½œï¼ŒFalse å¦‚æœä¸å¯ç”¨
    """
    # Check if Elasticsearch is explicitly disabled
    es_enabled = os.getenv("ENABLE_ELASTICSEARCH", "true").lower() != "false"
    if not es_enabled:
        logger.info("â„¹ï¸  Elasticsearch å·²è¢« ENABLE_ELASTICSEARCH=false åœç”¨ï¼Œè·³é ES ç´¢å¼•æ›´æ–°")
        return False
        
    if Elasticsearch is None:
        logger.info("â„¹ï¸  Elasticsearch å¥—ä»¶æœªå®‰è£ï¼Œè·³é ES ç´¢å¼•æ›´æ–°")
        return False
        
    es_host = os.getenv("ES_HOST", "localhost")
    es_port = int(os.getenv("ES_PORT", 9200))
    es_scheme = os.getenv("ES_SCHEME", "http")
    es_user = os.getenv("ES_USER")
    es_pass = os.getenv("ES_PASS")

    auth = (es_user, es_pass) if es_user and es_pass else None
    
    try:
        es = Elasticsearch([{"host": es_host, "port": es_port, "scheme": es_scheme}], http_auth=auth)
        
        # æ¸¬è©¦é€£ç·š
        if es.ping():
            logger.info(f"âœ… Elasticsearch å¯ç”¨: {es_host}:{es_port}")
            return True
        else:
            logger.info(f"â„¹ï¸  ç„¡æ³•é€£ç·šåˆ° Elasticsearch: {es_host}:{es_port}ï¼Œè·³é ES ç´¢å¼•æ›´æ–°")
            return False
            
    except Exception as e:
        logger.info(f"â„¹ï¸  Elasticsearch é€£ç·šå¤±æ•—: {e}ï¼Œè·³é ES ç´¢å¼•æ›´æ–°")
        return False


def _compare_es_document(es, es_index, db_obj):
    """
    æ¯”è¼ƒ Elasticsearch ä¸­çš„æ–‡ä»¶èˆ‡è³‡æ–™åº«è¨˜éŒ„æ˜¯å¦ä¸€è‡´
    è¿”å› True å¦‚æœéœ€è¦æ›´æ–°ï¼ŒFalse å¦‚æœå·²æ˜¯æœ€æ–°
    """
    try:
        es_doc = es.get(index=es_index, id=db_obj.ivod_id)
        es_source = es_doc['_source']
        
        # æ¯”è¼ƒé—œéµæ¬„ä½
        db_ai = db_obj.ai_transcript or ""
        db_ly = db_obj.ly_transcript or ""
        db_title = db_obj.title or ""
        
        es_ai = es_source.get('ai_transcript', "")
        es_ly = es_source.get('ly_transcript', "")
        es_title = es_source.get('title', "")
        
        # å¦‚æœä»»ä½•æ¬„ä½ä¸åŒï¼Œå°±éœ€è¦æ›´æ–°
        return not (db_ai == es_ai and db_ly == es_ly and db_title == es_title)
        
    except Exception:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–å…¶ä»–éŒ¯èª¤ï¼Œéœ€è¦ç´¢å¼•
        return True


def run_es(ivod_ids=None, full_mode=False):
    """
    æ™ºèƒ½æ›´æ–° Elasticsearch ç´¢å¼•ï¼š
    - æ¯”è¼ƒè³‡æ–™åº«èˆ‡ ES ä¸­çš„å…§å®¹ï¼Œåªæ›´æ–°æœ‰å·®ç•°çš„è¨˜éŒ„
    - æ”¯æ´æŒ‡å®š ivod_ids é€²è¡Œé¸æ“‡æ€§æ›´æ–°
    - æ”¯æ´ full_mode é€²è¡Œå®Œæ•´è³‡æ–™åº«æ¯”å°
    
    åƒæ•¸:
    - ivod_ids: å¯é¸çš„ IVOD ID åˆ—è¡¨ï¼Œåƒ…è™•ç†æŒ‡å®šçš„è¨˜éŒ„
    - full_mode: æ˜¯å¦é€²è¡Œå®Œæ•´è³‡æ–™åº«æ¯”å° (é è¨­ False)
    """
    setup_logging()
    
    # Check if Elasticsearch is explicitly disabled
    es_enabled = os.getenv("ENABLE_ELASTICSEARCH", "true").lower() != "false"
    if not es_enabled:
        logger.info("â„¹ï¸  Elasticsearch å·²è¢« ENABLE_ELASTICSEARCH=false åœç”¨ï¼Œè·³éç´¢å¼•æ›´æ–°")
        return True  # Return True since this is expected behavior, not an error
    
    if Elasticsearch is None:
        logger.error("âŒ Elasticsearch æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install elasticsearch")
        return False
        
    es_host = os.getenv("ES_HOST", "localhost")
    es_port = int(os.getenv("ES_PORT", 9200))
    es_scheme = os.getenv("ES_SCHEME", "http")
    es_user = os.getenv("ES_USER")
    es_pass = os.getenv("ES_PASS")
    es_index = os.getenv("ES_INDEX", "ivod_transcripts")

    auth = (es_user, es_pass) if es_user and es_pass else None
    
    try:
        es = Elasticsearch([{"host": es_host, "port": es_port, "scheme": es_scheme}], http_auth=auth)
        
        # æ¸¬è©¦é€£ç·š
        if not es.ping():
            logger.error(f"âŒ ç„¡æ³•é€£ç·šåˆ° Elasticsearch: {es_host}:{es_port}")
            return False
            
        logger.info(f"âœ… å·²é€£ç·šåˆ° Elasticsearch: {es_host}:{es_port}")
        
    except Exception as e:
        logger.error(f"âŒ Elasticsearch é€£ç·šå¤±æ•—: {e}")
        return False

    index_body = {
        "settings": {
            "analysis": {
                "analyzer": {
                    "chinese_analyzer": {
                        "tokenizer": "ik_max_word",
                        "filter": ["lowercase"]
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                "ivod_id": {"type": "integer"},
                "ai_transcript": {"type": "text", "analyzer": "chinese_analyzer"},
                "ly_transcript": {"type": "text", "analyzer": "chinese_analyzer"},
                "title": {"type": "text", "analyzer": "chinese_analyzer"},
                "last_updated": {"type": "date"}
            }
        }
    }

    # ç¢ºä¿ç´¢å¼•å­˜åœ¨
    try:
        if not es.indices.exists(index=es_index):
            es.indices.create(index=es_index, body=index_body)
            logger.info(f"âœ… å·²å‰µå»º Elasticsearch ç´¢å¼•: {es_index}")
        else:
            logger.info(f"âœ… Elasticsearch ç´¢å¼•å·²å­˜åœ¨: {es_index}")
    except Exception as e:
        logger.error(f"âŒ å‰µå»ºç´¢å¼•å¤±æ•—: {e}")
        return False

    db = Session()
    
    try:
        # æ±ºå®šè¦è™•ç†çš„è¨˜éŒ„
        if ivod_ids:
            # è™•ç†æŒ‡å®šçš„ IVOD IDs
            query = db.query(IVODTranscript).filter(IVODTranscript.ivod_id.in_(ivod_ids))
            desc = f"è™•ç†æŒ‡å®šçš„ {len(ivod_ids)} ç­†è¨˜éŒ„"
            logger.info(f"ğŸ” é¸æ“‡æ€§æ›´æ–°æ¨¡å¼: è™•ç† {len(ivod_ids)} ç­†æŒ‡å®šè¨˜éŒ„")
        elif full_mode:
            # å®Œæ•´è³‡æ–™åº«æ¯”å°æ¨¡å¼
            query = db.query(IVODTranscript)
            desc = "å®Œæ•´è³‡æ–™åº«æ¯”å°"
            logger.info("ğŸ” å®Œæ•´æ¯”å°æ¨¡å¼: æª¢æŸ¥æ‰€æœ‰è³‡æ–™åº«è¨˜éŒ„")
        else:
            # é è¨­ï¼šåªè™•ç†æœ€è¿‘æ›´æ–°çš„è¨˜éŒ„ (éå»7å¤©)
            seven_days_ago = datetime.now() - timedelta(days=7)
            query = db.query(IVODTranscript).filter(IVODTranscript.last_updated >= seven_days_ago)
            desc = "è™•ç†è¿‘æœŸæ›´æ–°è¨˜éŒ„"
            logger.info("ğŸ” å¢é‡æ›´æ–°æ¨¡å¼: è™•ç†éå»7å¤©æ›´æ–°çš„è¨˜éŒ„")
        
        records = query.all()
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(records)} ç­†å€™é¸è¨˜éŒ„")
        
        if not records:
            logger.info("â„¹ï¸  æ²’æœ‰è¨˜éŒ„éœ€è¦è™•ç†")
            return True
        
        # æ‰¹æ¬¡è™•ç†è¨˜éŒ„
        updated_count = 0
        skipped_count = 0
        error_count = 0
        batch_size = 100
        batch_docs = []
        
        for obj in tqdm(records, desc=desc):
            try:
                # æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                needs_update = _compare_es_document(es, es_index, obj)
                
                if not needs_update:
                    skipped_count += 1
                    continue
                
                # æº–å‚™æ–‡ä»¶å…§å®¹
                doc = {
                    "ivod_id": obj.ivod_id,
                    "ai_transcript": obj.ai_transcript or "",
                    "ly_transcript": obj.ly_transcript or "",
                    "title": obj.title or "",
                    "last_updated": obj.last_updated.isoformat() if obj.last_updated else None
                }
                
                batch_docs.append({
                    "index": {
                        "_index": es_index,
                        "_id": obj.ivod_id
                    }
                })
                batch_docs.append(doc)
                
                # ç•¶æ‰¹æ¬¡æ»¿äº†å°±åŸ·è¡Œæ‰¹æ¬¡ç´¢å¼•
                if len(batch_docs) >= batch_size * 2:  # æ¯å€‹æ–‡ä»¶æœ‰å…©å€‹é …ç›®
                    try:
                        response = es.bulk(body=batch_docs)
                        if response.get('errors'):
                            logger.warning(f"âš ï¸  æ‰¹æ¬¡ç´¢å¼•éƒ¨åˆ†å¤±æ•—")
                            for item in response['items']:
                                if 'index' in item and item['index'].get('error'):
                                    error_count += 1
                                    logger.error(f"ç´¢å¼•å¤±æ•— ID {item['index']['_id']}: {item['index']['error']}")
                                else:
                                    updated_count += 1
                        else:
                            updated_count += len(batch_docs) // 2
                        
                        batch_docs = []
                    except Exception as e:
                        logger.error(f"âŒ æ‰¹æ¬¡ç´¢å¼•å¤±æ•—: {e}")
                        error_count += len(batch_docs) // 2
                        batch_docs = []
                        
            except Exception as e:
                logger.error(f"âŒ è™•ç†è¨˜éŒ„ {obj.ivod_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                error_count += 1
                continue
        
        # è™•ç†æœ€å¾Œä¸€æ‰¹
        if batch_docs:
            try:
                response = es.bulk(body=batch_docs)
                if response.get('errors'):
                    logger.warning(f"âš ï¸  æœ€å¾Œæ‰¹æ¬¡ç´¢å¼•éƒ¨åˆ†å¤±æ•—")
                    for item in response['items']:
                        if 'index' in item and item['index'].get('error'):
                            error_count += 1
                            logger.error(f"ç´¢å¼•å¤±æ•— ID {item['index']['_id']}: {item['index']['error']}")
                        else:
                            updated_count += 1
                else:
                    updated_count += len(batch_docs) // 2
            except Exception as e:
                logger.error(f"âŒ æœ€å¾Œæ‰¹æ¬¡ç´¢å¼•å¤±æ•—: {e}")
                error_count += len(batch_docs) // 2
        
        # è¨˜éŒ„çµ±è¨ˆçµæœ
        logger.info(f"âœ… Elasticsearch ç´¢å¼•æ›´æ–°å®Œæˆ:")
        logger.info(f"   - å·²æ›´æ–°: {updated_count} ç­†")
        logger.info(f"   - å·²è·³é: {skipped_count} ç­† (å…§å®¹ç›¸åŒ)")
        logger.info(f"   - å¤±æ•—: {error_count} ç­†")
        logger.info(f"   - ç¸½è¨ˆè™•ç†: {updated_count + skipped_count + error_count} ç­†")
        
        return error_count == 0
        
    finally:
        db.close()