#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ivod_tasks.py

å°‡ fullã€incrementalã€retry ä¸‰ç¨®ä¸»è¦å·¥ä½œæµç¨‹é›†ä¸­åœ¨æ­¤ï¼Œä¾› ivod_full.pyã€ivod_incremental.pyã€ivod_retry.py å‘¼å«ã€‚
"""
import json
import logging
from datetime import datetime, timedelta
import os
from pathlib import Path

from tqdm import tqdm
try:
    from elasticsearch import Elasticsearch
except ImportError:
    Elasticsearch = None

from .core import date_range, make_browser, fetch_ivod_list, process_ivod
from .db import (
    DB_BACKEND, engine, Base, Session, IVODTranscript,
    check_and_create_database_tables,
    check_elasticsearch_available, run_elasticsearch_indexing
)

logger = logging.getLogger(__name__)

# check_and_create_database_tables å‡½æ•¸å·²ç§»è‡³ db.py ä¸­

def log_failed_ivod(ivod_id, error_type="general"):
    """è¨˜éŒ„å¤±æ•—çš„IVOD_IDåˆ°éŒ¯èª¤æ—¥èªŒæª”æ¡ˆ"""
    error_log_path = os.getenv("ERROR_LOG_PATH", "logs/failed_ivods.txt")
    error_dir = Path(error_log_path).parent
    error_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(error_log_path, "a", encoding="utf-8") as f:
        f.write(f"{ivod_id},{error_type},{timestamp}\n")

def setup_logging():
    """è¨­ç½®æ—¥èªŒé…ç½® - æˆåŠŸæ¶ˆæ¯åªè¨˜éŒ„åˆ°æ–‡ä»¶ï¼ŒéŒ¯èª¤æ¶ˆæ¯åŒæ™‚é¡¯ç¤ºåœ¨æ§åˆ¶å°å’Œè¨˜éŒ„åˆ°æ–‡ä»¶"""
    log_path = os.getenv("LOG_PATH", "logs/")
    log_dir = Path(log_path)
    log_dir.mkdir(exist_ok=True)
    
    log_file = log_dir / f"crawler_{datetime.now().strftime('%Y%m%d')}.log"
    
    # æ¸…é™¤ç¾æœ‰çš„handlersä»¥é¿å…é‡è¤‡è¨­ç½®
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # å‰µå»ºæ–‡ä»¶handlerï¼Œè¨˜éŒ„æ‰€æœ‰ç´šåˆ¥çš„æ—¥èªŒ
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s")
    file_handler.setFormatter(file_formatter)
    
    # å‰µå»ºæ§åˆ¶å°handlerï¼Œåªé¡¯ç¤ºERRORå’ŒWARNINGç´šåˆ¥
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.WARNING)
    console_formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s")
    console_handler.setFormatter(console_formatter)
    
    # é…ç½®root logger
    logging.basicConfig(
        level=logging.INFO,
        handlers=[file_handler, console_handler]
    )


def run_full(skip_ssl: bool = True, start_date: str = None, end_date: str = None):
    """
    å…¨é‡æ‹‰å–ï¼šå¾æŒ‡å®šèµ·å§‹æ—¥è·‘åˆ°æŒ‡å®šçµæŸæ—¥ï¼ˆæˆ–ä»Šå¤©ï¼‰ï¼Œé€ç­† upsert åˆ°è³‡æ–™åº«ã€‚
    
    Args:
        skip_ssl: æ˜¯å¦è·³éSSLé©—è­‰
        start_date: è‡ªè¨‚èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)ï¼Œå¦‚æœæ—©æ–¼é è¨­èµ·å§‹æ—¥æœŸå‰‡ä½¿ç”¨é è¨­å€¼
        end_date: è‡ªè¨‚çµæŸæ—¥æœŸ (YYYY-MM-DD)ï¼Œå¦‚æœæ™šæ–¼ä»Šå¤©å‰‡ä½¿ç”¨ä»Šå¤©
    """
    setup_logging()
    
    # æª¢æŸ¥ä¸¦ç¢ºä¿è³‡æ–™åº«è¡¨æ ¼å­˜åœ¨
    logger.info("ğŸ” æª¢æŸ¥è³‡æ–™åº«ç‹€æ³...")
    if not check_and_create_database_tables():
        logger.error("âŒ è³‡æ–™åº«æª¢æŸ¥å¤±æ•—ï¼Œåœæ­¢åŸ·è¡Œ")
        return False
    
    br = make_browser(skip_ssl=skip_ssl)
    db = Session()

    # é è¨­èµ·å§‹å’ŒçµæŸæ—¥æœŸ
    default_start = "2024-02-01"
    today = datetime.now().strftime("%Y-%m-%d")
    
    # é©—è­‰å’Œè¨­å®šå¯¦éš›ä½¿ç”¨çš„æ—¥æœŸç¯„åœ
    actual_start = _validate_date_range(start_date, end_date, default_start, today)
    actual_end = _validate_date_range(start_date, end_date, default_start, today, is_end_date=True)
    
    start, end = actual_start, actual_end
    
    logger.info(f"ğŸ“… æ—¥æœŸç¯„åœï¼š{start} è‡³ {end}")
    if start_date and start_date != actual_start:
        logger.warning(f"âš ï¸  èµ·å§‹æ—¥æœŸ {start_date} æ—©æ–¼é è¨­èµ·å§‹æ—¥æœŸï¼Œä½¿ç”¨ {actual_start}")
    if end_date and end_date != actual_end:
        logger.warning(f"âš ï¸  çµæŸæ—¥æœŸ {end_date} æ™šæ–¼ä»Šå¤©ï¼Œä½¿ç”¨ {actual_end}")
    for date_str in tqdm(date_range(start, end), desc="æ—¥æœŸ"):
        try:
            ids = fetch_ivod_list(br, date_str)
        except Exception as e:
            logger.error(f"{date_str} åˆ—è¡¨å¤±æ•—: {e}")
            continue

        for ivod_id in tqdm(ids, desc=f"{date_str} å½±ç‰‡", leave=False):
            try:
                logger.info(f"è™•ç†å½±ç‰‡ {ivod_id}")
                rec = process_ivod(br, ivod_id)
                
                # Check if record exists
                obj = db.query(IVODTranscript).filter_by(ivod_id=ivod_id).first()
                if obj:
                    # Update existing record
                    for k, v in rec.items():
                        setattr(obj, k, v)
                    obj.last_updated = datetime.now()
                else:
                    # Create new record
                    rec["last_updated"] = datetime.now()
                    db.add(IVODTranscript(**rec))
                
                # Commit this single record
                db.commit()
                logger.info(f"å½±ç‰‡ {ivod_id} è™•ç†å®Œæˆ")
                
            except Exception as e:
                logger.error(f"è™•ç†å½±ç‰‡ {ivod_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
                # Rollback any pending changes for this record
                db.rollback()
                log_failed_ivod(ivod_id, "processing")
                continue
    db.close()
    logger.info("å…¨é‡æ‹‰å–å®Œæˆã€‚")
    
    # æª¢æŸ¥ Elasticsearch æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœå¯ç”¨å°±è‡ªå‹•æ›´æ–°ç´¢å¼•
    if check_elasticsearch_available():
        logger.info("ğŸ”„ é–‹å§‹è‡ªå‹•æ›´æ–° Elasticsearch ç´¢å¼•...")
        try:
            es_success = run_elasticsearch_indexing(full_mode=True)
            if es_success:
                logger.info("âœ… Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°å®Œæˆ")
            else:
                logger.warning("âš ï¸  Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°å¤±æ•—")
        except Exception as e:
            logger.warning(f"âš ï¸  Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    return True


def run_incremental(skip_ssl: bool = True):
    """
    å¢é‡æ›´æ–°ï¼šåªæª¢æŸ¥éå»å…©é€±çš„æ–° IDï¼Œä¸¦é‡å°ç¼ºæ¼çš„ AI æˆ– LY é€å­—ç¨¿é€²è¡Œè£œæŠ“ã€‚
    """
    setup_logging()
    
    # æª¢æŸ¥ä¸¦ç¢ºä¿è³‡æ–™åº«è¡¨æ ¼å­˜åœ¨
    logger.info("ğŸ” æª¢æŸ¥è³‡æ–™åº«ç‹€æ³...")
    if not check_and_create_database_tables():
        logger.error("âŒ è³‡æ–™åº«æª¢æŸ¥å¤±æ•—ï¼Œåœæ­¢åŸ·è¡Œ")
        return False
    
    br = make_browser(skip_ssl=skip_ssl)
    db = Session()

    today = datetime.now().date()
    two_weeks_ago = today - timedelta(days=14)
    ids = set()
    for d in date_range(two_weeks_ago.isoformat(), today.isoformat()):
        try:
            ids.update(fetch_ivod_list(br, d))
        except Exception:
            continue

    for ivod_id in tqdm(ids, desc="å¢é‡æ›´æ–°å½±ç‰‡"):
        try:
            logger.info(f"å¢é‡æ›´æ–°å½±ç‰‡ {ivod_id}")
            obj = db.get(IVODTranscript, ivod_id)
            if not obj:
                rec = process_ivod(br, ivod_id)
                rec["last_updated"] = datetime.now()
                db.add(IVODTranscript(**rec))
                logger.info(f"æ–°å¢å½±ç‰‡ {ivod_id}")
                continue
            if not obj.ai_transcript:
                rec = process_ivod(br, ivod_id)
                obj.ai_transcript = rec["ai_transcript"]
                obj.last_updated = datetime.now()
                logger.info(f"æ›´æ–°å½±ç‰‡ {ivod_id} AIé€å­—ç¨¿")
            if not obj.ly_transcript:
                rec = process_ivod(br, ivod_id)
                obj.ly_transcript = rec["ly_transcript"]
                obj.last_updated = datetime.now()
                logger.info(f"æ›´æ–°å½±ç‰‡ {ivod_id} LYé€å­—ç¨¿")
        except Exception as e:
            logger.error(f"å¢é‡æ›´æ–°å½±ç‰‡ {ivod_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            log_failed_ivod(ivod_id, "incremental")
            continue

    db.commit()
    db.close()
    logger.info("å¢é‡æ›´æ–°å®Œæˆã€‚")
    
    # æª¢æŸ¥ Elasticsearch æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœå¯ç”¨å°±è‡ªå‹•æ›´æ–°ç´¢å¼•ï¼ˆå¢é‡æ¨¡å¼ï¼‰
    if check_elasticsearch_available():
        logger.info("ğŸ”„ é–‹å§‹è‡ªå‹•æ›´æ–° Elasticsearch ç´¢å¼•ï¼ˆå¢é‡æ¨¡å¼ï¼‰...")
        try:
            es_success = run_elasticsearch_indexing()  # ä½¿ç”¨é è¨­å¢é‡æ¨¡å¼
            if es_success:
                logger.info("âœ… Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°å®Œæˆ")
            else:
                logger.warning("âš ï¸  Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°å¤±æ•—")
        except Exception as e:
            logger.warning(f"âš ï¸  Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    return True


def run_retry(skip_ssl: bool = True):
    """
    é‡æ–°å˜—è©¦å¤±æ•—çš„ä»»å‹™ï¼šAI æˆ– LY é€å­—ç¨¿ä¹‹å‰ç™¼ç”ŸéŒ¯èª¤ï¼Œä¸”é‡è©¦æ¬¡æ•¸å°šæœªè¶…éä¸Šé™ã€‚
    """
    setup_logging()
    
    # æª¢æŸ¥ä¸¦ç¢ºä¿è³‡æ–™åº«è¡¨æ ¼å­˜åœ¨
    logger.info("ğŸ” æª¢æŸ¥è³‡æ–™åº«ç‹€æ³...")
    if not check_and_create_database_tables():
        logger.error("âŒ è³‡æ–™åº«æª¢æŸ¥å¤±æ•—ï¼Œåœæ­¢åŸ·è¡Œ")
        return False
    
    br = make_browser(skip_ssl=skip_ssl)
    db = Session()

    MAX_RETRIES = 5
    to_retry = db.query(IVODTranscript).filter(
        IVODTranscript.ai_status == 'failed',
        IVODTranscript.ai_retries < MAX_RETRIES
    ).all()
    to_retry += db.query(IVODTranscript).filter(
        IVODTranscript.ly_status == 'failed',
        IVODTranscript.ly_retries < MAX_RETRIES
    ).all()

    # è¨˜éŒ„æˆåŠŸé‡è©¦çš„ IVOD IDs
    successfully_retried_ids = []

    for obj in to_retry:
        try:
            logger.info(f"é‡è©¦å½±ç‰‡ {obj.ivod_id}")
            rec = process_ivod(br, obj.ivod_id)
            # Update the existing object with new data
            for k, v in rec.items():
                setattr(obj, k, v)
            obj.last_updated = datetime.now()
            db.commit()
            successfully_retried_ids.append(obj.ivod_id)
            logger.info(f"é‡è©¦å½±ç‰‡ {obj.ivod_id} å®Œæˆ")
        except Exception as e:
            logger.error(f"é‡è©¦å½±ç‰‡ {obj.ivod_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            log_failed_ivod(obj.ivod_id, "retry")
            continue

    db.close()
    logger.info("Retry ä»»å‹™å®Œæˆã€‚")
    
    # æª¢æŸ¥ Elasticsearch æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœå¯ç”¨ä¸”æœ‰æˆåŠŸé‡è©¦çš„è¨˜éŒ„å°±è‡ªå‹•æ›´æ–°ç´¢å¼•
    if successfully_retried_ids and check_elasticsearch_available():
        logger.info(f"ğŸ”„ é–‹å§‹è‡ªå‹•æ›´æ–° Elasticsearch ç´¢å¼•ï¼ˆé‡è©¦çš„ {len(successfully_retried_ids)} ç­†è¨˜éŒ„ï¼‰...")
        try:
            es_success = run_elasticsearch_indexing(ivod_ids=successfully_retried_ids)
            if es_success:
                logger.info("âœ… Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°å®Œæˆ")
            else:
                logger.warning("âš ï¸  Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°å¤±æ•—")
        except Exception as e:
            logger.warning(f"âš ï¸  Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    elif successfully_retried_ids:
        logger.info(f"â„¹ï¸  å·²é‡è©¦ {len(successfully_retried_ids)} ç­†è¨˜éŒ„ï¼Œä½† Elasticsearch ä¸å¯ç”¨")


def read_failed_ivods_from_file(error_log_path):
    """å¾éŒ¯èª¤è¨˜éŒ„æª”æ¡ˆè®€å–å¤±æ•—çš„IVOD_IDåˆ—è¡¨"""
    failed_ivods = []
    if not os.path.exists(error_log_path):
        logger.warning(f"éŒ¯èª¤è¨˜éŒ„æª”æ¡ˆä¸å­˜åœ¨: {error_log_path}")
        return failed_ivods
    
    with open(error_log_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                parts = line.split(',')
                if len(parts) >= 1:
                    try:
                        ivod_id = int(parts[0])
                        failed_ivods.append(ivod_id)
                    except ValueError:
                        logger.warning(f"ç„¡æ•ˆçš„IVOD_IDæ ¼å¼: {parts[0]}")
    
    # å»é‡è¤‡
    return list(set(failed_ivods))


def remove_from_error_log(ivod_id, error_log_path):
    """å¾éŒ¯èª¤è¨˜éŒ„æª”æ¡ˆä¸­ç§»é™¤æˆåŠŸè™•ç†çš„IVOD_ID"""
    if not os.path.exists(error_log_path):
        return
    
    # è®€å–ç¾æœ‰è¨˜éŒ„
    lines = []
    with open(error_log_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    # éæ¿¾æ‰æŒ‡å®šçš„IVOD_ID
    filtered_lines = []
    for line in lines:
        if line.strip():
            parts = line.strip().split(',')
            if len(parts) >= 1 and parts[0] != str(ivod_id):
                filtered_lines.append(line)
    
    # å¯«å›æª”æ¡ˆ
    with open(error_log_path, "w", encoding="utf-8") as f:
        f.writelines(filtered_lines)


def run_fix(ivod_ids=None, error_log_path=None, skip_ssl: bool = True):
    """
    ä¿®å¾©å¤±æ•—çš„IVODè¨˜éŒ„
    
    Args:
        ivod_ids: æŒ‡å®šè¦ä¿®å¾©çš„IVOD_IDåˆ—è¡¨ï¼Œå¦‚æœç‚ºNoneä¸”error_log_pathç‚ºNoneå‰‡ä½¿ç”¨é è¨­éŒ¯èª¤è¨˜éŒ„æª”æ¡ˆ
        error_log_path: éŒ¯èª¤è¨˜éŒ„æª”æ¡ˆè·¯å¾‘ï¼Œå¦‚æœæŒ‡å®šå‰‡å¾æª”æ¡ˆè®€å–å¤±æ•—çš„IVOD_IDåˆ—è¡¨
        skip_ssl: æ˜¯å¦è·³éSSLé©—è­‰
    
    Returns:
        bool: åŸ·è¡Œæ˜¯å¦æˆåŠŸ
    """
    logger.info("é–‹å§‹ Fix ä»»å‹™...")
    
    br = make_browser(skip_ssl=skip_ssl)
    db = Session()
    
    # ç¢ºå®šè¦ä¿®å¾©çš„IVODåˆ—è¡¨
    if ivod_ids:
        # ç›´æ¥ä½¿ç”¨æŒ‡å®šçš„IVOD_IDåˆ—è¡¨
        target_ivods = ivod_ids
        logger.info(f"æŒ‡å®šä¿®å¾© {len(target_ivods)} å€‹IVODè¨˜éŒ„")
        
    else:
        # å¾éŒ¯èª¤è¨˜éŒ„æª”æ¡ˆè®€å–
        if not error_log_path:
            error_log_path = os.getenv("ERROR_LOG_PATH", "logs/failed_ivods.txt")
        
        target_ivods = read_failed_ivods_from_file(error_log_path)
        if not target_ivods:
            logger.info("æ²’æœ‰æ‰¾åˆ°éœ€è¦ä¿®å¾©çš„IVODè¨˜éŒ„")
            db.close()
            return True
        
        logger.info(f"å¾ {error_log_path} æ‰¾åˆ° {len(target_ivods)} å€‹éœ€è¦ä¿®å¾©çš„IVODè¨˜éŒ„")
    
    success_count = 0
    failed_count = 0
    successfully_fixed_ids = []
    
    try:
        for ivod_id in tqdm(target_ivods, desc="ä¿®å¾©IVODè¨˜éŒ„"):
            try:
                logger.info(f"é–‹å§‹è™•ç†IVOD_ID: {ivod_id}")
                rec = process_ivod(br, ivod_id)
                
                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨è¨˜éŒ„
                obj = db.get(IVODTranscript, ivod_id)
                if obj:
                    # æ›´æ–°ç¾æœ‰è¨˜éŒ„
                    for k, v in rec.items():
                        setattr(obj, k, v)
                    obj.last_updated = datetime.now()
                    logger.info(f"æ›´æ–°IVOD {ivod_id} æˆåŠŸ")
                else:
                    # æ–°å¢è¨˜éŒ„
                    rec["last_updated"] = datetime.now()
                    db.add(IVODTranscript(**rec))
                    logger.info(f"æ–°å¢IVOD {ivod_id} æˆåŠŸ")
                
                db.commit()
                success_count += 1
                successfully_fixed_ids.append(ivod_id)
                
                # å¦‚æœå¾éŒ¯èª¤è¨˜éŒ„æª”æ¡ˆè®€å–çš„ï¼Œç§»é™¤æˆåŠŸè™•ç†çš„è¨˜éŒ„
                if not ivod_ids and error_log_path:
                    remove_from_error_log(ivod_id, error_log_path)
                
            except Exception as e:
                logger.error(f"è™•ç†IVOD {ivod_id} å¤±æ•—: {e}", exc_info=True)
                db.rollback()
                failed_count += 1
                # é‡æ–°è¨˜éŒ„å¤±æ•—
                log_failed_ivod(ivod_id, "fix_retry")
                continue
        
    finally:
        db.close()
    
    logger.info(f"ä¿®å¾©å®Œæˆ - æˆåŠŸ: {success_count}, å¤±æ•—: {failed_count}")
    
    # æª¢æŸ¥ Elasticsearch æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœå¯ç”¨ä¸”æœ‰æˆåŠŸä¿®å¾©çš„è¨˜éŒ„å°±æ‰¹é‡æ›´æ–°ç´¢å¼•
    if successfully_fixed_ids and check_elasticsearch_available():
        logger.info(f"ğŸ”„ é–‹å§‹è‡ªå‹•æ›´æ–° Elasticsearch ç´¢å¼•ï¼ˆä¿®å¾©çš„ {len(successfully_fixed_ids)} ç­†è¨˜éŒ„ï¼‰...")
        try:
            es_success = run_elasticsearch_indexing(ivod_ids=successfully_fixed_ids)
            if es_success:
                logger.info("âœ… Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°å®Œæˆ")
            else:
                logger.warning("âš ï¸  Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°å¤±æ•—")
        except Exception as e:
            logger.warning(f"âš ï¸  Elasticsearch ç´¢å¼•è‡ªå‹•æ›´æ–°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    elif successfully_fixed_ids:
        logger.info(f"â„¹ï¸  å·²ä¿®å¾© {len(successfully_fixed_ids)} ç­†è¨˜éŒ„ï¼Œä½† Elasticsearch ä¸å¯ç”¨")
    
    logger.info("Fix ä»»å‹™å®Œæˆã€‚")
    return True


def run_backup(backup_file=None):
    """
    å‚™ä»½è³‡æ–™åº«å…§å®¹åˆ° JSON æª”æ¡ˆ
    
    Args:
        backup_file: å‚™ä»½æª”æ¡ˆè·¯å¾‘ï¼Œå¦‚æœç‚º None å‰‡è‡ªå‹•ç”Ÿæˆ
    
    Returns:
        str: å‚™ä»½æª”æ¡ˆè·¯å¾‘ï¼Œå¤±æ•—æ™‚è¿”å› None
    """
    logger.info("é–‹å§‹è³‡æ–™åº«å‚™ä»½...")
    
    db = Session()
    
    try:
        # è‡ªå‹•ç”Ÿæˆå‚™ä»½æª”æ¡ˆå
        if not backup_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = f"backup/ivod_backup_{timestamp}.json"
        
        # ç¢ºä¿å‚™ä»½ç›®éŒ„å­˜åœ¨
        backup_dir = os.path.dirname(backup_file)
        if backup_dir and not os.path.exists(backup_dir):
            os.makedirs(backup_dir)
            logger.info(f"å»ºç«‹å‚™ä»½ç›®éŒ„: {backup_dir}")
        
        # æŸ¥è©¢æ‰€æœ‰è¨˜éŒ„
        records = db.query(IVODTranscript).all()
        record_count = len(records)
        
        if record_count == 0:
            logger.warning("è³‡æ–™åº«ä¸­æ²’æœ‰è¨˜éŒ„å¯å‚™ä»½")
            return None
        
        logger.info(f"æ‰¾åˆ° {record_count} ç­†è¨˜éŒ„ï¼Œé–‹å§‹å‚™ä»½...")
        
        # è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        backup_data = {
            "metadata": {
                "backup_time": datetime.now().isoformat(),
                "db_backend": DB_BACKEND,
                "record_count": record_count,
                "version": "1.0"
            },
            "data": []
        }
        
        for record in tqdm(records, desc="å‚™ä»½è¨˜éŒ„"):
            record_dict = {
                "ivod_id": record.ivod_id,
                "ivod_url": record.ivod_url,
                "date": record.date.isoformat() if record.date else None,
                "meeting_code": record.meeting_code,
                "meeting_code_str": record.meeting_code_str,
                "meeting_name": record.meeting_name,
                "meeting_time": record.meeting_time.isoformat() if record.meeting_time else None,
                "title": record.title,
                "speaker_name": record.speaker_name,
                "video_length": record.video_length,
                "video_start": record.video_start,
                "video_end": record.video_end,
                "video_type": record.video_type,
                "category": record.category,
                "video_url": record.video_url,
                "committee_names": record.committee_names,
                "ai_transcript": record.ai_transcript,
                "ai_status": record.ai_status,
                "ai_retries": record.ai_retries,
                "ly_transcript": record.ly_transcript,
                "ly_status": record.ly_status,
                "ly_retries": record.ly_retries,
                "last_updated": record.last_updated.isoformat() if record.last_updated else None
            }
            backup_data["data"].append(record_dict)
        
        # å¯«å…¥ JSON æª”æ¡ˆ
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, ensure_ascii=False, indent=2)
        
        file_size = os.path.getsize(backup_file) / (1024 * 1024)  # MB
        logger.info(f"âœ… å‚™ä»½å®Œæˆ: {backup_file}")
        logger.info(f"ğŸ“Š å‚™ä»½çµ±è¨ˆ: {record_count} ç­†è¨˜éŒ„ï¼Œæª”æ¡ˆå¤§å°: {file_size:.2f} MB")
        
        return backup_file
        
    except Exception as e:
        logger.error(f"å‚™ä»½å¤±æ•—: {e}", exc_info=True)
        return None
    finally:
        db.close()


def run_restore(backup_file, force_create_table=False, force_clear_data=False):
    """
    å¾å‚™ä»½æª”æ¡ˆé‚„åŸè³‡æ–™åº«
    
    Args:
        backup_file: å‚™ä»½æª”æ¡ˆè·¯å¾‘
        force_create_table: å¼·åˆ¶å»ºç«‹è³‡æ–™è¡¨ï¼ˆä¸è©¢å•ï¼‰
        force_clear_data: å¼·åˆ¶æ¸…é™¤ç¾æœ‰è³‡æ–™ï¼ˆä¸è©¢å•ï¼‰
    
    Returns:
        bool: é‚„åŸæ˜¯å¦æˆåŠŸ
    """
    logger.info(f"é–‹å§‹å¾å‚™ä»½æª”æ¡ˆé‚„åŸ: {backup_file}")
    
    # æª¢æŸ¥å‚™ä»½æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(backup_file):
        logger.error(f"å‚™ä»½æª”æ¡ˆä¸å­˜åœ¨: {backup_file}")
        return False
    
    try:
        # è®€å–å‚™ä»½æª”æ¡ˆ
        with open(backup_file, 'r', encoding='utf-8') as f:
            backup_data = json.load(f)
        
        # é©—è­‰å‚™ä»½æª”æ¡ˆæ ¼å¼
        if "metadata" not in backup_data or "data" not in backup_data:
            logger.error("å‚™ä»½æª”æ¡ˆæ ¼å¼éŒ¯èª¤")
            return False
        
        metadata = backup_data["metadata"]
        records_data = backup_data["data"]
        
        logger.info(f"ğŸ“Š å‚™ä»½æª”æ¡ˆè³‡è¨Š:")
        logger.info(f"   - å‚™ä»½æ™‚é–“: {metadata.get('backup_time')}")
        logger.info(f"   - åŸå§‹è³‡æ–™åº«: {metadata.get('db_backend')}")
        logger.info(f"   - è¨˜éŒ„æ•¸é‡: {metadata.get('record_count')}")
        logger.info(f"   - ç‰ˆæœ¬: {metadata.get('version')}")
        
        db = Session()
        
        try:
            # æª¢æŸ¥è³‡æ–™è¡¨æ˜¯å¦å­˜åœ¨
            table_exists = _check_table_exists(db)
            
            if not table_exists:
                if not force_create_table:
                    response = input("è³‡æ–™è¡¨ä¸å­˜åœ¨ï¼Œæ˜¯å¦è¦å»ºç«‹ï¼Ÿ(y/N): ").strip().lower()
                    if response not in ['y', 'yes']:
                        logger.info("ä½¿ç”¨è€…å–æ¶ˆå»ºç«‹è³‡æ–™è¡¨")
                        return False
                
                logger.info("å»ºç«‹è³‡æ–™è¡¨...")
                from .db import Base, engine
                Base.metadata.create_all(engine)
                logger.info("âœ… è³‡æ–™è¡¨å»ºç«‹å®Œæˆ")
            
            # æª¢æŸ¥ç¾æœ‰è³‡æ–™
            existing_count = db.query(IVODTranscript).count()
            
            if existing_count > 0:
                logger.warning(f"è³‡æ–™åº«ä¸­å·²æœ‰ {existing_count} ç­†è¨˜éŒ„")
                
                if not force_clear_data:
                    response = input("æ˜¯å¦è¦æ¸…é™¤ç¾æœ‰è³‡æ–™ï¼Ÿ(y/N): ").strip().lower()
                    if response not in ['y', 'yes']:
                        logger.info("ä½¿ç”¨è€…é¸æ“‡ä¿ç•™ç¾æœ‰è³‡æ–™ï¼Œé‚„åŸå–æ¶ˆ")
                        return False
                
                logger.info("æ¸…é™¤ç¾æœ‰è³‡æ–™...")
                db.query(IVODTranscript).delete()
                db.commit()
                logger.info("âœ… ç¾æœ‰è³‡æ–™å·²æ¸…é™¤")
            
            # é‚„åŸè³‡æ–™
            logger.info(f"é–‹å§‹é‚„åŸ {len(records_data)} ç­†è¨˜éŒ„...")
            
            success_count = 0
            error_count = 0
            
            for record_data in tqdm(records_data, desc="é‚„åŸè¨˜éŒ„"):
                try:
                    # è½‰æ›æ—¥æœŸå­—æ®µ
                    if record_data.get("date"):
                        record_data["date"] = datetime.fromisoformat(record_data["date"]).date()
                    
                    if record_data.get("meeting_time"):
                        record_data["meeting_time"] = datetime.fromisoformat(record_data["meeting_time"])
                    
                    if record_data.get("last_updated"):
                        record_data["last_updated"] = datetime.fromisoformat(record_data["last_updated"])
                    
                    # å»ºç«‹è¨˜éŒ„
                    record = IVODTranscript(**record_data)
                    db.add(record)
                    success_count += 1
                    
                except Exception as e:
                    logger.error(f"é‚„åŸè¨˜éŒ„å¤±æ•— (IVOD_ID: {record_data.get('ivod_id')}): {e}")
                    error_count += 1
                    continue
            
            # æäº¤æ‰€æœ‰è®Šæ›´
            db.commit()
            
            logger.info(f"âœ… é‚„åŸå®Œæˆ")
            logger.info(f"ğŸ“Š é‚„åŸçµ±è¨ˆ: æˆåŠŸ {success_count} ç­†ï¼Œå¤±æ•— {error_count} ç­†")
            
            return True
            
        except Exception as e:
            logger.error(f"é‚„åŸéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            db.rollback()
            return False
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"è®€å–å‚™ä»½æª”æ¡ˆå¤±æ•—: {e}", exc_info=True)
        return False


def _check_table_exists(db):
    """æª¢æŸ¥ IVODTranscript è³‡æ–™è¡¨æ˜¯å¦å­˜åœ¨"""
    try:
        # å˜—è©¦æŸ¥è©¢è³‡æ–™è¡¨ï¼Œå¦‚æœä¸å­˜åœ¨æœƒæ‹‹å‡ºç•°å¸¸
        db.query(IVODTranscript).limit(1).first()
        return True
    except Exception:
        return False


def _validate_date_range(start_date, end_date, default_start, today, is_end_date=False):
    """
    é©—è­‰æ—¥æœŸç¯„åœçš„è¼”åŠ©å‡½æ•¸
    
    Args:
        start_date: ä½¿ç”¨è€…æŒ‡å®šçš„èµ·å§‹æ—¥æœŸ
        end_date: ä½¿ç”¨è€…æŒ‡å®šçš„çµæŸæ—¥æœŸ
        default_start: é è¨­èµ·å§‹æ—¥æœŸ
        today: ä»Šå¤©çš„æ—¥æœŸ
        is_end_date: æ˜¯å¦ç‚ºçµæŸæ—¥æœŸé©—è­‰
    
    Returns:
        str: é©—è­‰å¾Œçš„æ—¥æœŸ
    """
    if is_end_date:
        # é©—è­‰çµæŸæ—¥æœŸ
        if not end_date:
            return today
        
        # æª¢æŸ¥æ—¥æœŸæ ¼å¼
        try:
            end_dt = datetime.strptime(end_date, "%Y-%m-%d")
            today_dt = datetime.strptime(today, "%Y-%m-%d")
        except ValueError:
            logger.error(f"âŒ çµæŸæ—¥æœŸæ ¼å¼éŒ¯èª¤: {end_date}ï¼Œæ‡‰ç‚º YYYY-MM-DD")
            return today
        
        # ä¸å¯æ™šæ–¼ä»Šå¤©
        if end_dt > today_dt:
            return today
        
        return end_date
    
    else:
        # é©—è­‰èµ·å§‹æ—¥æœŸ
        if not start_date:
            return default_start
        
        # æª¢æŸ¥æ—¥æœŸæ ¼å¼
        try:
            start_dt = datetime.strptime(start_date, "%Y-%m-%d")
            default_dt = datetime.strptime(default_start, "%Y-%m-%d")
        except ValueError:
            logger.error(f"âŒ èµ·å§‹æ—¥æœŸæ ¼å¼éŒ¯èª¤: {start_date}ï¼Œæ‡‰ç‚º YYYY-MM-DD")
            return default_start
        
        # ä¸å¯æ—©æ–¼é è¨­èµ·å§‹æ—¥æœŸ
        if start_dt < default_dt:
            return default_start
        
        return start_date


# Elasticsearch functions have been moved to db.py
# This maintains backward compatibility for existing code
def run_es(ivod_ids=None, full_mode=False):
    """
    èˆŠç‰ˆ run_es å‡½æ•¸çš„ç›¸å®¹æ€§åŒ…è£å™¨
    å¯¦éš›åŠŸèƒ½å·²ç§»è‡³ db.py çš„ run_elasticsearch_indexing å‡½æ•¸
    """
    setup_logging()
    return run_elasticsearch_indexing(ivod_ids=ivod_ids, full_mode=full_mode)