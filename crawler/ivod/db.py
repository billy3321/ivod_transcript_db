
# 1. Configure DB URL from environment
import os
from dotenv import load_dotenv
from .database_env import get_database_config, get_database_environment, print_database_info

# Load environment variables from .env file
load_dotenv()

# ç²å–è³‡æ–™åº«ç’°å¢ƒè¨­å®š
db_config = get_database_config()
db_env = get_database_environment()

# è¨­å®šè³‡æ–™åº«é€£ç·š
DB_BACKEND = os.getenv("DB_BACKEND", "sqlite").lower()
DB_URL = db_config["url"]

# åœ¨éç”Ÿç”¢ç’°å¢ƒé¡¯ç¤ºè³‡æ–™åº«ç’°å¢ƒè³‡è¨Š
if os.getenv("ENVIRONMENT") != "production":
    print_database_info()

# 2. SQLAlchemy setup
from sqlalchemy import (
    create_engine, Column, Integer, Text, Date, ARRAY, TIMESTAMP, JSON
)
from sqlalchemy.dialects.mysql import LONGTEXT
from sqlalchemy.dialects.postgresql import TEXT as PG_TEXT
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

engine = create_engine(DB_URL, echo=False)
Session = sessionmaker(bind=engine)
Base = declarative_base()

# 3. ORM Model
class IVODTranscript(Base):
    __tablename__ = "ivod_transcripts"
    ivod_id          = Column(Integer, primary_key=True)
    ivod_url         = Column(Text, nullable=False)
    date             = Column(Date,  nullable=False)
    meeting_code     = Column(Text)
    meeting_code_str = Column(Text)
    category         = Column(Text)
    video_type       = Column(Text)
    video_start      = Column(Text)
    video_end        = Column(Text)
    video_length     = Column(Text)
    video_url        = Column(Text)
    title            = Column(Text)
    speaker_name     = Column(Text)
    meeting_time     = Column(TIMESTAMP(timezone=True)) if DB_BACKEND!="sqlite" else Column(Text)
    meeting_name     = Column(Text)
    # Use appropriate text type for large content based on backend
    if DB_BACKEND == "mysql":
        ai_transcript = Column(LONGTEXT)
        ly_transcript = Column(LONGTEXT) 
    elif DB_BACKEND == "postgresql":
        ai_transcript = Column(PG_TEXT)  # PostgreSQL TEXT has no length limit
        ly_transcript = Column(PG_TEXT)
    else:  # sqlite
        ai_transcript = Column(Text)  # SQLite TEXT has no length limit
        ly_transcript = Column(Text)
    ai_status  = Column(Text,   nullable=False, default="pending")
    ai_retries = Column(Integer, nullable=False, default=0)
    ly_status  = Column(Text,   nullable=False, default="pending")
    ly_retries = Column(Integer, nullable=False, default=0)
    last_updated     = Column(TIMESTAMP(timezone=True), nullable=False) if DB_BACKEND!="sqlite" else Column(Text, nullable=False)
    if DB_BACKEND == "postgresql":
        committee_names = Column(ARRAY(Text))
    elif DB_BACKEND == "mysql":
        committee_names = Column(JSON)
    else:  # sqlite
        # SQLite does not support ARRAY
        committee_names = Column(Text)

import logging
from datetime import datetime, timedelta
from pathlib import Path
from tqdm import tqdm

try:
    from elasticsearch import Elasticsearch
except ImportError:
    Elasticsearch = None

logger = logging.getLogger(__name__)

# Database management functions
def check_and_create_database_tables():
    """
    æª¢æŸ¥è³‡æ–™åº«é€£ç·šç‹€æ³ä¸¦ç¢ºä¿è¡¨æ ¼å­˜åœ¨
    """
    from sqlalchemy import inspect, text
    
    try:
        # æª¢æŸ¥è³‡æ–™åº«é€£ç·š
        with engine.connect() as conn:
            result = conn.execute(text("SELECT 1"))
            logger.info(f"âœ… è³‡æ–™åº«é€£ç·šæˆåŠŸ (Backend: {DB_BACKEND})")
        
        # æª¢æŸ¥è¡¨æ ¼æ˜¯å¦å­˜åœ¨
        inspector = inspect(engine)
        tables = inspector.get_table_names()
        
        if 'ivod_transcripts' not in tables:
            logger.info("âš ï¸  ivod_transcripts è¡¨æ ¼ä¸å­˜åœ¨ï¼Œæ­£åœ¨å‰µå»º...")
            Base.metadata.create_all(engine)
            logger.info("âœ… è¡¨æ ¼å‰µå»ºæˆåŠŸ")
        else:
            logger.info("âœ… ivod_transcripts è¡¨æ ¼å·²å­˜åœ¨")
            
            # æª¢æŸ¥è¡¨æ ¼çµæ§‹
            columns = inspector.get_columns('ivod_transcripts')
            logger.info(f"âœ… è¡¨æ ¼åŒ…å« {len(columns)} å€‹æ¬„ä½")
            
            # æª¢æŸ¥ç¾æœ‰è¨˜éŒ„æ•¸
            with Session() as session:
                count = session.query(IVODTranscript).count()
                logger.info(f"âœ… ç¾æœ‰è¨˜éŒ„æ•¸: {count}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ è³‡æ–™åº«æª¢æŸ¥å¤±æ•—: {e}")
        return False

# Elasticsearch management functions
def check_elasticsearch_available():
    """
    æª¢æŸ¥ Elasticsearch æ˜¯å¦å¯ç”¨
    è¿”å› True å¦‚æœ ES æ­£å¸¸é‹ä½œï¼ŒFalse å¦‚æœä¸å¯ç”¨
    """
    # Check if Elasticsearch is explicitly disabled
    es_enabled = os.getenv("ENABLE_ELASTICSEARCH", "true").lower() != "false"
    if not es_enabled:
        logger.info("â„¹ï¸  Elasticsearch å·²è¢« ENABLE_ELASTICSEARCH=false åœç”¨ï¼Œè·³é ES ç´¢å¼•æ›´æ–°")
        return False
        
    if Elasticsearch is None:
        logger.info("â„¹ï¸  Elasticsearch å¥—ä»¶æœªå®‰è£ï¼Œè·³é ES ç´¢å¼•æ›´æ–°")
        return False
        
    from .database_env import get_elasticsearch_config
    
    es_config = get_elasticsearch_config()
    auth = (es_config["user"], es_config["password"]) if es_config["user"] and es_config["password"] else None
    
    try:
        es = Elasticsearch([{"host": es_config["host"], "port": es_config["port"], "scheme": es_config["scheme"]}], http_auth=auth)
        
        # æ¸¬è©¦é€£ç·š
        if es.ping():
            logger.info(f"âœ… Elasticsearch å¯ç”¨: {es_config['host']}:{es_config['port']}")
            return True
        else:
            logger.info(f"â„¹ï¸  ç„¡æ³•é€£ç·šåˆ° Elasticsearch: {es_config['host']}:{es_config['port']}ï¼Œè·³é ES ç´¢å¼•æ›´æ–°")
            return False
            
    except Exception as e:
        logger.info(f"â„¹ï¸  Elasticsearch é€£ç·šå¤±æ•—: {e}ï¼Œè·³é ES ç´¢å¼•æ›´æ–°")
        return False

def get_elasticsearch_client():
    """
    å»ºç«‹ä¸¦è¿”å› Elasticsearch å®¢æˆ¶ç«¯
    è¿”å› (es_client, es_index) æˆ– (None, None) å¦‚æœé€£ç·šå¤±æ•—
    """
    if Elasticsearch is None:
        logger.error("âŒ Elasticsearch æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install elasticsearch")
        return None, None
        
    from .database_env import get_elasticsearch_config
    
    es_config = get_elasticsearch_config()
    auth = (es_config["user"], es_config["password"]) if es_config["user"] and es_config["password"] else None
    
    try:
        es = Elasticsearch([{"host": es_config["host"], "port": es_config["port"], "scheme": es_config["scheme"]}], http_auth=auth)
        
        # æ¸¬è©¦é€£ç·š
        if not es.ping():
            logger.error(f"âŒ ç„¡æ³•é€£ç·šåˆ° Elasticsearch: {es_config['host']}:{es_config['port']}")
            return None, None
            
        logger.info(f"âœ… å·²é€£ç·šåˆ° Elasticsearch: {es_config['host']}:{es_config['port']}")
        return es, es_config["index"]
        
    except Exception as e:
        logger.error(f"âŒ Elasticsearch é€£ç·šå¤±æ•—: {e}")
        return None, None

def create_elasticsearch_index(es, es_index):
    """
    å»ºç«‹ Elasticsearch ç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    """
    index_body = {
        "settings": {
            "analysis": {
                "analyzer": {
                    "chinese_analyzer": {
                        "tokenizer": "ik_max_word",
                        "filter": ["lowercase"]
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                "ivod_id": {"type": "integer"},
                "ai_transcript": {"type": "text", "analyzer": "chinese_analyzer"},
                "ly_transcript": {"type": "text", "analyzer": "chinese_analyzer"},
                "title": {"type": "text", "analyzer": "chinese_analyzer"},
                "last_updated": {"type": "date"}
            }
        }
    }

    try:
        if not es.indices.exists(index=es_index):
            es.indices.create(index=es_index, body=index_body)
            logger.info(f"âœ… å·²å‰µå»º Elasticsearch ç´¢å¼•: {es_index}")
        else:
            logger.info(f"âœ… Elasticsearch ç´¢å¼•å·²å­˜åœ¨: {es_index}")
        return True
    except Exception as e:
        logger.error(f"âŒ å‰µå»ºç´¢å¼•å¤±æ•—: {e}")
        return False

def compare_es_document(es, es_index, db_obj):
    """
    æ¯”è¼ƒ Elasticsearch ä¸­çš„æ–‡ä»¶èˆ‡è³‡æ–™åº«è¨˜éŒ„æ˜¯å¦ä¸€è‡´
    è¿”å› True å¦‚æœéœ€è¦æ›´æ–°ï¼ŒFalse å¦‚æœå·²æ˜¯æœ€æ–°
    """
    try:
        es_doc = es.get(index=es_index, id=db_obj.ivod_id)
        es_source = es_doc['_source']
        
        # æ¯”è¼ƒé—œéµæ¬„ä½
        db_ai = db_obj.ai_transcript or ""
        db_ly = db_obj.ly_transcript or ""
        db_title = db_obj.title or ""
        
        es_ai = es_source.get('ai_transcript', "")
        es_ly = es_source.get('ly_transcript', "")
        es_title = es_source.get('title', "")
        
        # å¦‚æœä»»ä½•æ¬„ä½ä¸åŒï¼Œå°±éœ€è¦æ›´æ–°
        return not (db_ai == es_ai and db_ly == es_ly and db_title == es_title)
        
    except Exception:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–å…¶ä»–éŒ¯èª¤ï¼Œéœ€è¦ç´¢å¼•
        return True

def bulk_index_to_elasticsearch(es, es_index, records, batch_size=100):
    """
    æ‰¹é‡ç´¢å¼•è¨˜éŒ„åˆ° Elasticsearch
    
    è¿”å› (updated_count, skipped_count, error_count)
    """
    updated_count = 0
    skipped_count = 0
    error_count = 0
    batch_docs = []
    
    for obj in tqdm(records, desc="è™•ç†è¨˜éŒ„"):
        try:
            # æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            needs_update = compare_es_document(es, es_index, obj)
            
            if not needs_update:
                skipped_count += 1
                continue
            
            # æº–å‚™æ–‡ä»¶å…§å®¹
            doc = {
                "ivod_id": obj.ivod_id,
                "ai_transcript": obj.ai_transcript or "",
                "ly_transcript": obj.ly_transcript or "",
                "title": obj.title or "",
                "last_updated": obj.last_updated.isoformat() if obj.last_updated else None
            }
            
            batch_docs.append({
                "index": {
                    "_index": es_index,
                    "_id": obj.ivod_id
                }
            })
            batch_docs.append(doc)
            
            # ç•¶æ‰¹æ¬¡æ»¿äº†å°±åŸ·è¡Œæ‰¹æ¬¡ç´¢å¼•
            if len(batch_docs) >= batch_size * 2:  # æ¯å€‹æ–‡ä»¶æœ‰å…©å€‹é …ç›®
                try:
                    response = es.bulk(body=batch_docs)
                    if response.get('errors'):
                        logger.warning(f"âš ï¸  æ‰¹æ¬¡ç´¢å¼•éƒ¨åˆ†å¤±æ•—")
                        for item in response['items']:
                            if 'index' in item and item['index'].get('error'):
                                error_count += 1
                                logger.error(f"ç´¢å¼•å¤±æ•— ID {item['index']['_id']}: {item['index']['error']}")
                            else:
                                updated_count += 1
                    else:
                        updated_count += len(batch_docs) // 2
                    
                    batch_docs = []
                except Exception as e:
                    logger.error(f"âŒ æ‰¹æ¬¡ç´¢å¼•å¤±æ•—: {e}")
                    error_count += len(batch_docs) // 2
                    batch_docs = []
                    
        except Exception as e:
            logger.error(f"âŒ è™•ç†è¨˜éŒ„ {obj.ivod_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            error_count += 1
            continue
    
    # è™•ç†æœ€å¾Œä¸€æ‰¹
    if batch_docs:
        try:
            response = es.bulk(body=batch_docs)
            if response.get('errors'):
                logger.warning(f"âš ï¸  æœ€å¾Œæ‰¹æ¬¡ç´¢å¼•éƒ¨åˆ†å¤±æ•—")
                for item in response['items']:
                    if 'index' in item and item['index'].get('error'):
                        error_count += 1
                        logger.error(f"ç´¢å¼•å¤±æ•— ID {item['index']['_id']}: {item['index']['error']}")
                    else:
                        updated_count += 1
            else:
                updated_count += len(batch_docs) // 2
        except Exception as e:
            logger.error(f"âŒ æœ€å¾Œæ‰¹æ¬¡ç´¢å¼•å¤±æ•—: {e}")
            error_count += len(batch_docs) // 2
    
    return updated_count, skipped_count, error_count

def run_elasticsearch_indexing(ivod_ids=None, full_mode=False):
    """
    æ™ºèƒ½æ›´æ–° Elasticsearch ç´¢å¼•ï¼š
    - æ¯”è¼ƒè³‡æ–™åº«èˆ‡ ES ä¸­çš„å…§å®¹ï¼Œåªæ›´æ–°æœ‰å·®ç•°çš„è¨˜éŒ„
    - æ”¯æ´æŒ‡å®š ivod_ids é€²è¡Œé¸æ“‡æ€§æ›´æ–°
    - æ”¯æ´ full_mode é€²è¡Œå®Œæ•´è³‡æ–™åº«æ¯”å°
    
    åƒæ•¸:
    - ivod_ids: å¯é¸çš„ IVOD ID åˆ—è¡¨ï¼Œåƒ…è™•ç†æŒ‡å®šçš„è¨˜éŒ„
    - full_mode: æ˜¯å¦é€²è¡Œå®Œæ•´è³‡æ–™åº«æ¯”å° (é è¨­ False)
    
    è¿”å›:
    - bool: æ˜¯å¦æˆåŠŸåŸ·è¡Œï¼ˆç„¡éŒ¯èª¤ï¼‰
    """
    # Check if Elasticsearch is explicitly disabled
    es_enabled = os.getenv("ENABLE_ELASTICSEARCH", "true").lower() != "false"
    if not es_enabled:
        logger.info("â„¹ï¸  Elasticsearch å·²è¢« ENABLE_ELASTICSEARCH=false åœç”¨ï¼Œè·³éç´¢å¼•æ›´æ–°")
        return True  # Return True since this is expected behavior, not an error
    
    # å»ºç«‹ Elasticsearch é€£ç·š
    es, es_index = get_elasticsearch_client()
    if not es:
        return False
    
    # ç¢ºä¿ç´¢å¼•å­˜åœ¨
    if not create_elasticsearch_index(es, es_index):
        return False
    
    db = Session()
    
    try:
        # æ±ºå®šè¦è™•ç†çš„è¨˜éŒ„
        if ivod_ids:
            # è™•ç†æŒ‡å®šçš„ IVOD IDs
            query = db.query(IVODTranscript).filter(IVODTranscript.ivod_id.in_(ivod_ids))
            desc = f"è™•ç†æŒ‡å®šçš„ {len(ivod_ids)} ç­†è¨˜éŒ„"
            logger.info(f"ğŸ” é¸æ“‡æ€§æ›´æ–°æ¨¡å¼: è™•ç† {len(ivod_ids)} ç­†æŒ‡å®šè¨˜éŒ„")
        elif full_mode:
            # å®Œæ•´è³‡æ–™åº«æ¯”å°æ¨¡å¼
            query = db.query(IVODTranscript)
            desc = "å®Œæ•´è³‡æ–™åº«æ¯”å°"
            logger.info("ğŸ” å®Œæ•´æ¯”å°æ¨¡å¼: æª¢æŸ¥æ‰€æœ‰è³‡æ–™åº«è¨˜éŒ„")
        else:
            # é è¨­ï¼šåªè™•ç†æœ€è¿‘æ›´æ–°çš„è¨˜éŒ„ (éå»7å¤©)
            seven_days_ago = datetime.now() - timedelta(days=7)
            query = db.query(IVODTranscript).filter(IVODTranscript.last_updated >= seven_days_ago)
            desc = "è™•ç†è¿‘æœŸæ›´æ–°è¨˜éŒ„"
            logger.info("ğŸ” å¢é‡æ›´æ–°æ¨¡å¼: è™•ç†éå»7å¤©æ›´æ–°çš„è¨˜éŒ„")
        
        records = query.all()
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(records)} ç­†å€™é¸è¨˜éŒ„")
        
        if not records:
            logger.info("â„¹ï¸  æ²’æœ‰è¨˜éŒ„éœ€è¦è™•ç†")
            return True
        
        # æ‰¹é‡è™•ç†è¨˜éŒ„
        updated_count, skipped_count, error_count = bulk_index_to_elasticsearch(es, es_index, records)
        
        # è¨˜éŒ„çµ±è¨ˆçµæœ
        logger.info(f"âœ… Elasticsearch ç´¢å¼•æ›´æ–°å®Œæˆ:")
        logger.info(f"   - å·²æ›´æ–°: {updated_count} ç­†")
        logger.info(f"   - å·²è·³é: {skipped_count} ç­† (å…§å®¹ç›¸åŒ)")
        logger.info(f"   - å¤±æ•—: {error_count} ç­†")
        logger.info(f"   - ç¸½è¨ˆè™•ç†: {updated_count + skipped_count + error_count} ç­†")
        
        return error_count == 0
        
    finally:
        db.close()

# Note: Tables will be created by check_and_create_database_tables() function
# when needed, rather than automatically on module import